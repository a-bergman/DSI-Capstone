{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering, Modeling, & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "import matplotlib.pyplot     as plt\n",
    "import seaborn               as sns\n",
    "import xgboost               as xgb\n",
    "from math                    import sqrt\n",
    "from sklearn.linear_model    import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics         import r2_score, mean_squared_error, mean_absolute_error \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from IPython.core.display    import display, HTML\n",
    "from IPython.display         import display_html\n",
    "sns.set(style = \"white\", palette = \"husl\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "\n",
    "1. [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "\n",
    "\n",
    "2. [Feature Engineering](#Feature-Engineering)\n",
    "    - [Transforming Numeric Data](#Transforming-Numeric-Data)\n",
    "    - [Creating Segmental Features](#Creating-Segmental-Features)\n",
    "    \n",
    "\n",
    "3. [Modeling](#Modeling)\n",
    "    - [Functions](#Functions)\n",
    "    - [Linear Regression](#Linear-Regression)\n",
    "    - [Ridge](#Ridge)\n",
    "    - [LASSO](#LASSO)\n",
    "    - [ElasticNetCV](#ElasticNetCV)\n",
    "    - [Random Forest Reression](#Random-Forest-Regression)\n",
    "    - [Prediction Dataframes](#Prediction-Dataframes)\n",
    "    \n",
    "\n",
    "4. [Model Evaluation](#Model-Evaluation)\n",
    "    - [Functions](#Evaluation-Functions)\n",
    "    - [Plotting Predictions](#Plotting-Predictions)\n",
    "    - [Plotting Residuals](#Plotting-Residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri = pd.read_csv(\"../Data/mri_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "\n",
    "print(f\"The shape of the dataset is: {mri.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of column data types\n",
    "\n",
    "mri.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Numeric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only four numeric columns in the data set: `age`, `lvesv`, `lvedv`, `lvef`.  Of the four, only `lvef` does not have any kind of a normal distribution: `age` is close to normally distributed, while `lvesv` and `lvedv` are log-normally distributed.\n",
    "\n",
    "We cannot do anything to `lvedv` because that is my target variable, but we can take the log of `lvesv` (in this case the natural log).  We also tried squaring `age` but that did not affect the distribution in the way we hoped it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the natural log of `lvesv`.\n",
    "# We chose to make it it's own column rather\n",
    "# than overwrite the column.\n",
    "\n",
    "mri[\"lvesv_log\"] = mri[\"lvesv\"].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the dataset is: {mri.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Segmental Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model attempts to predict the end diastolic volume, we want it to be as accurate as it can be.  As part of that, we will try to use different combinations of features in an attempt to achieve high accuracy.\n",
    "\n",
    "The data have 34 columns that we wish to engineer: a column measuring scarification and a column measuring ischemia.  Because there are so many of them, we felt the need to experiment with how they are passed into the model.  We are unable to create multiplication interaction columns, because there are zeros.  Instead, we elected to create segmental columns by summing similar columns together: we will compare the model's performance with the originals and with the segmental columns.\n",
    "\n",
    "We used this image to guide our create of segmental columns:\n",
    "\n",
    "<img src = \"../Images/cardiac-segmentation-for-cardiac-perfusion-defects.jpg\" alt = \"Cardiac Segmentation\" height = 750 width = 750>\n",
    "\n",
    "Case courtesy of Dr Hamid Chalian, <a href=\"https://radiopaedia.org/\">Radiopaedia.org</a>. From the case <a href=\"https://radiopaedia.org/cases/47102\">rID: 47102</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns based off of the red, blue, green, and yellow\n",
    "# We combined the apex and apical regions because there is only\n",
    "# one region in the apex.\n",
    "\n",
    "# New columns for scar tissue\n",
    "\n",
    "mri[\"basal_he\"]    = mri[\"ba_he\"] + mri[\"bas_he\"] + mri[\"bis_he\"] \\\n",
    "                     + mri[\"bi_he\"] + mri[\"bil_he\"] + mri[\"bal_he\"]\n",
    "mri[\"mid_he\"]      = mri[\"ma_he\"] + mri[\"mas_he\"] + mri[\"mis_he\"] \\\n",
    "                     + mri[\"mi_he\"] + mri[\"mil_he\"] + mri[\"mal_he\"]\n",
    "mri[\"apical_he\"]   = mri[\"aa_he\"] + mri[\"as_he\"] + mri[\"ai_he\"] \\\n",
    "                     + mri[\"al_he\"] + mri[\"apex_he\"]\n",
    "\n",
    "# New columns for ischemia\n",
    "\n",
    "mri[\"basal_ischemia\"]  = mri[\"ba_ischemia\"] + mri[\"bas_ischemia\"] + mri[\"bis_ischemia\"] \\\n",
    "                         + mri[\"bi_ischemia\"] + mri[\"bil_ischemia\"] + mri[\"bal_ischemia\"]\n",
    "mri[\"mid_ischemia\"]    = mri[\"ma_ischemia\"] + mri[\"mas_ischemia\"] + mri[\"mis_ischemia\"] \\\n",
    "                         + mri[\"mi_ischemia\"] + mri[\"mil_ischemia\"] + mri[\"mal_ischemia\"]\n",
    "mri[\"apical_ischemia\"] = mri[\"aa_ischemia\"] + mri[\"as_ischemia\"] + mri[\"ai_ischemia\"] \\\n",
    "                         + mri[\"al_ischemia\"]\n",
    "\n",
    "print(f\"The shape of the dataset is: {mri.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intended to create new columns based off the secondary axis (anteroseptal, inferoseptal, etc.) but because of how the apical region is divided we would have counted the apical regions more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our goals was to see what would happen if we used the engineered features to reduce the overall number of reatures being passed into the model.  To accomplish that, we created two dataframes: one with the original set of features (48 including the target) and a second one with the engineered features (21 including the target).\n",
    "\n",
    "\n",
    "From this point on, `_org` will be used to refer to the original set of features and `_eng` will be use to refer to the engineered features.  The default will be `_org`, so after train-splitting the originial will just be `X_train` or `X_train_ss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the original dataframe\n",
    "\n",
    "mri_org = mri.drop(labels = [\"lvesv_log\", \"basal_he\", \"mid_he\",\n",
    "                             \"apical_he\", \"basal_ischemia\",\n",
    "                             \"mid_ischemia\", \"apical_ischemia\"],\n",
    "                   axis = 1)\n",
    "\n",
    "# Defining the dataframe with only new features\n",
    "\n",
    "mri_eng = mri.drop(labels = ['lvesv', 'ba_he', 'bas_he', 'bis_he','bi_he', \n",
    "                             'bil_he', 'bal_he', 'ma_he', 'mas_he', 'mis_he', \n",
    "                             'mi_he', 'mil_he','mal_he', 'aa_he', 'as_he', \n",
    "                             'ai_he', 'al_he', 'apex_he', 'ba_ischemia',\n",
    "                             'bas_ischemia', 'bis_ischemia', 'bi_ischemia', \n",
    "                             'bil_ischemia','bal_ischemia', 'ma_ischemia', \n",
    "                             'mas_ischemia', 'mis_ischemia','mi_ischemia', \n",
    "                             'mil_ischemia', 'mal_ischemia', 'aa_ischemia', \n",
    "                             'as_ischemia', 'ai_ischemia', 'al_ischemia'],\n",
    "                   axis = 1)\n",
    "\n",
    "# Checking to make sure the two have different numbers of columns\n",
    "\n",
    "print(f\"The shape of the dataframe with original features is  : {mri_org.shape}\")\n",
    "print(f\"The shape of the dataframe with engineered features is: {mri_eng.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start modeling, we have to perform a train-test split.  A train-test split allows us to train our data on one subset of the dataframe and train on another subset.\n",
    "\n",
    "Since I have two versions of the dataframe, I will have to train-test split on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up my X and y variables for the original\n",
    "\n",
    "X = mri_org.drop(\"lvedv\", axis = 1)\n",
    "y = mri_org[\"lvedv\"]\n",
    "\n",
    "# Setting up my X and y variables for the new\n",
    "\n",
    "X_eng = mri_eng.drop(\"lvedv\", axis = 1)\n",
    "y_eng = mri_eng[\"lvedv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test splitting mri_og\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    test_size    = 0.25) \n",
    "\n",
    "# Train-test splitting mri_new\n",
    "\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(X_eng, \n",
    "                                                                    y_eng,\n",
    "                                                                    random_state = 42,\n",
    "                                                                    test_size    = 0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model going forward will be fit and evaluated on both sets of train-test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will be running models that need to have the data scaled, I will run both sets of X variables through `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the standard scaler\n",
    "\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the set with original \n",
    "# features.\n",
    "\n",
    "# Fit-transforming the X_train features\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "\n",
    "# Transforming my X_test variables\n",
    "\n",
    "X_test_ss  = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the sub-set with the\n",
    "# features I engineered\n",
    "\n",
    "# Fit-transforming the engineered X_train\n",
    "\n",
    "X_train_eng_ss = ss.fit_transform(X_train_eng)\n",
    "\n",
    "# Transforming the engineered X_test features\n",
    "\n",
    "X_test_eng_ss  = ss.transform(X_test_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula to generate an adjusted r^2 score for\n",
    "# my model evaluation function in the next cell\n",
    "\n",
    "# This formula was take from the linear regression lab\n",
    "\n",
    "def r2_adj(X, y_true, y_predicted):\n",
    "    r2          = r2_score(y_true, y_predicted)\n",
    "    numerator   = (1 - r2) * (len(y) - 1)\n",
    "    denominator = (len(y) - len(X.columns)) - 1\n",
    "    quotient    = numerator / denominator\n",
    "    r2_adj      = 1 - quotient\n",
    "    return r2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model_evaluation` function has four metrics:\n",
    "\n",
    "* r<sup>2</sup> to measure variance in `y` caused by the `X` variables;\n",
    "\n",
    "\n",
    "* adjusted r<sup>2</sup> to measure the same, but weighted for the number of features;\n",
    "\n",
    "\n",
    "* root mean squared error (RMSE) to measure the difference between my predicted values and the actual values;\n",
    "\n",
    "\n",
    "* mean absolute error (MAE) to measure the absolute mean error (this method is less sensitive to outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(X, y_true, y_predicted):\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_predicted))\n",
    "    mae  = mean_absolute_error(y_true, y_predicted)\n",
    "    r2   = r2_score(y_true, y_predicted)\n",
    "    print(f\"The root mean squared error is : {rmse}\")\n",
    "    print(f\"The mean absolute error is     : {mae}\")\n",
    "    print(f\"The r2 score is                : {r2}\")\n",
    "    print(f\"The adjusted r2 score is       : {r2_adj(X, y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the cell above, but lacking the adjusted r^2 score\n",
    "# because after standard scaling the `X` variables the columns are lost.\n",
    "\n",
    "def model_evaluation_nor2adj(y_true, y_predicted):\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_predicted))\n",
    "    mae  = mean_absolute_error(y_true, y_predicted)\n",
    "    r2   = r2_score(y_true, y_predicted)\n",
    "    print(f\"The root mean squared error is : {rmse}\")\n",
    "    print(f\"The mean absolute error is     : {mae}\")\n",
    "    print(f\"The r2 score is                : {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from user ntg on StackOverflow.  [Here](https://stackoverflow.com/a/44923103) is the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows me to display the dataframes of the linear regression\n",
    "# coefficients side-by-side to save space.\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace('table', 'table style=\"display:inline\"'), raw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When thinking about our approach to modeling, we decided to start with the simplest model we could think of which was the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original set is the dataframe with its original 48 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the linear regression\n",
    "\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the linear regression to the original\n",
    "# (non-engineered) subset\n",
    "\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions from the training set\n",
    "\n",
    "train_preds = lr.predict(X_train)\n",
    "\n",
    "# Generating my test predictions\n",
    "\n",
    "lr_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_train, y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_evaluation(X_test, y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of how similar the scores are, it is clear that the model is performing very well.\n",
    "\n",
    "The model is technically overfit, but it is so minor we decided to ignore the overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe makes looking the coefficients easier\n",
    "\n",
    "lin_reg_org = pd.DataFrame(data    = lr.coef_,\n",
    "                           index   = X_test.columns,\n",
    "                           columns = [\"Lin_Reg_Org\"])\n",
    "\n",
    "# Sorting for the head and tail five coefficients\n",
    "\n",
    "head = lin_reg_org.sort_values(by = \"Lin_Reg_Org\", ascending = False).head()\n",
    "tail = lin_reg_org.sort_values(by = \"Lin_Reg_Org\", ascending = False).tail()\n",
    "\n",
    "display_side_by_side(head,tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the heads and tails displays showed us the strongest and weakest coefficients as determined during the modeling process.\n",
    "\n",
    "We found it interesting that sex and smoker status were so strong, but hyperlipidemia was so weak in comparison: hicgh cholesterol is a significant factor for cardiac disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engineered subset is the dataframe but with the features we engineered instead of the original 34 segmental features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the linear model\n",
    "\n",
    "lr_2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the linear regression to the subset\n",
    "# with the features I engineered\n",
    "\n",
    "lr_2.fit(X_train_eng, y_train_eng);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the training set\n",
    "\n",
    "train_preds = lr_2.predict(X_train_eng)\n",
    "\n",
    "# Generating my predictions for the testing set\n",
    "\n",
    "lr_2_preds  = lr_2.predict(X_test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_train_eng, y_train_eng, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_test_eng, y_test_eng, lr_2_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the engineered subset are worse than for the original dataset, but they are even more consistant.\n",
    "\n",
    "This was firm evidence for us that a linear type model likely is the best way to model this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easier to view the coefficients in a df\n",
    "\n",
    "lin_reg_eng = pd.DataFrame(data  = lr_2.coef_,\n",
    "                           index = X_test_eng.columns,\n",
    "                           columns = [\"Lin_Reg_Eng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = lin_reg_eng.sort_values(by = \"Lin_Reg_Eng\", ascending = False).head()\n",
    "tail = lin_reg_eng.sort_values(by = \"Lin_Reg_Eng\", ascending = False).tail()\n",
    "\n",
    "display_side_by_side(head,tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression works by imposing a penalty on the coefficients: it uses the $\\ell$<sub>2</sub> regularization which shrinks thems towards 0 and effectively removes features without actually doing so and reduces the overall complexity.\n",
    "\n",
    "\n",
    "Because the features can be so strongly affected by the regularization it is important to have them all be on the same scale; this was done aboe with `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the ridge model\n",
    "\n",
    "ridge = RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ridge model to the training data\n",
    "\n",
    "ridge.fit(X_train_ss, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the training set\n",
    "\n",
    "train_preds = ridge.predict(X_train_ss)\n",
    "\n",
    "# Generating Predictions for the testing set\n",
    "\n",
    "ridge_preds = ridge.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase the RidgeCV and following models use scaled data, they lose the feature columns and instead use arrays of data.  For this reason, an adjusted r<sup>2</sup> cannot be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test, ridge_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Ridge model's performance is very similar to the linear model's performance which was surprising to us because we had expected the regularization to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the ridge model\n",
    "\n",
    "ridge_2 = RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ridge model to the data\n",
    "# with engineered features.\n",
    "\n",
    "ridge_2.fit(X_train_eng_ss, y_train_eng);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the trianing data\n",
    "\n",
    "train_preds = ridge_2.predict(X_train_eng_ss)\n",
    "\n",
    "# Generating Predictions\n",
    "\n",
    "ridge_2_preds = ridge_2.predict(X_test_eng_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train_eng, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test_eng, ridge_2_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO (Least Absolute Shrinkage and Selection Operator) is conceptually similar to Ridge in that it penalizes coefficients, but it uses $\\ell$<sub>2</sub> regularization which actually reduces the coefficients of unimportant features to exactly 0.\n",
    "\n",
    "LASSO also needs to have the features on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the LASSO model\n",
    "\n",
    "lasso = LassoCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the lasso to my training data\n",
    "# without engineered features\n",
    "\n",
    "lasso.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the training data\n",
    "\n",
    "train_preds = lasso.predict(X_train_ss)\n",
    "\n",
    "# Generating predictions for the testing data\n",
    "\n",
    "lasso_preds = lasso.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test, lasso_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the lasso model\n",
    "\n",
    "lasso_2 = LassoCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the lasso to my training data\n",
    "# with engineered features\n",
    "\n",
    "lasso_2.fit(X_train_eng_ss, y_train_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the training data\n",
    "\n",
    "train_preds = lasso_2.predict(X_train_eng_ss)\n",
    "\n",
    "# Generating predictions for the testing data\n",
    "\n",
    "lasso_2_preds = lasso_2.predict(X_test_eng_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test_eng, lasso_2_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net models are similar to both Ridge _and_ LASSO models: they are a compromise between the $\\ell$<sub>1</sub> and$\\ell$<sub>2</sub> regularizations.  In practice the model drives down non-significant features but does not remove them entirely.\n",
    "\n",
    "-----\n",
    "\n",
    "Like the Ridge and LASSO models, an ElasticNet needs to have the data be on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "\n",
    "enet = ElasticNetCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the \n",
    "# original data set\n",
    "\n",
    "enet.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "train_preds = enet.predict(X_train_ss)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "enet_preds = enet.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test, enet_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiatig the model\n",
    "\n",
    "enet_2 = ElasticNetCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the data\n",
    "# with engineered features\n",
    "\n",
    "enet_2.fit(X_train_eng_ss, y_train_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "train_preds = enet_2.predict(X_train_eng_ss)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "enet_2_preds = enet_2.predict(X_test_eng_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_train_eng, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_nor2adj(y_test_eng, enet_2_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest regressor works by bagging (bootstrap aggregation) random samples of the original dataframe and uses a metric (in our case MSE) to determine the quality of a split.  When it comes to predicting, it generates the mean of each node.\n",
    "\n",
    "We chose to use a pipeline to make it easier for us to find optimal hyperparameter values; hyperparameters are parameters defined by the individual running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the pipeline\n",
    "\n",
    "rfr_pipe = Pipeline([(\"rfr\", RandomForestRegressor(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the values for the gridsearch\n",
    "# These are the optimized values\n",
    "\n",
    "params = {\"rfr__n_estimators\"     : [35],\n",
    "          \"rfr__max_depth\"        : [None],\n",
    "          \"rfr__min_samples_split\": [10],\n",
    "          \"rfr__min_samples_leaf\" : [2,],\n",
    "          \"rfr__n_jobs\"           : [6]}\n",
    "\n",
    "# Instantiating the gridsearch\n",
    "\n",
    "rfr_gs = GridSearchCV(rfr_pipe, \n",
    "                      param_grid = params,\n",
    "                      cv         = 5)\n",
    "\n",
    "# Fitting the gridsearch to the training data\n",
    "\n",
    "rfr_gs.fit(X_train, y_train);\n",
    "\n",
    "# Getting the best parameters\n",
    "\n",
    "rfr_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "train_preds = rfr_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "rfr_preds = rfr_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_train, y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_test, y_test, rfr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the values for the gridsearch\n",
    "# These are the optimized values\n",
    "\n",
    "params = {\"rfr__n_estimators\"     : [35],\n",
    "          \"rfr__max_depth\"        : [None],\n",
    "          \"rfr__min_samples_split\": [2],\n",
    "          \"rfr__min_samples_leaf\" : [2],\n",
    "          \"rfr__n_jobs\"           : [6]}\n",
    "\n",
    "# Instantiating the gridsearch\n",
    "\n",
    "rfr_2_gs = GridSearchCV(rfr_pipe, \n",
    "                        param_grid = params,\n",
    "                        cv         = 5)\n",
    "\n",
    "# Fitting the gridsearch to the training data\n",
    "\n",
    "rfr_2_gs.fit(X_train_eng, y_train_eng);\n",
    "\n",
    "# Getting the best parameters\n",
    "\n",
    "rfr_2_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "train_preds = rfr_2_gs.predict(X_train_eng)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "rfr_2_preds = rfr_2_gs.predict(X_test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_train_eng, y_train_eng, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_test_eng, y_test_eng, rfr_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe out of the four sets of predictions\n",
    "# and the y_test true values for comparison\n",
    "\n",
    "org_model_predictions = pd.DataFrame([np.array(y_test), lr_preds, ridge_preds, \n",
    "                                      lasso_preds, enet_preds, rfr_preds])\n",
    "\n",
    "# Transposing the dataframe so it has five columns\n",
    "\n",
    "org_model_predictions = org_model_predictions.T\n",
    "\n",
    "# Renaming the columns for each model\n",
    "\n",
    "org_model_predictions.rename({0: \"Actual\", 1: \"Linear Reg.\", 2: \"RidgeCV\",\n",
    "                              3: \"LassoCV\", 4: \"ElasticNetCV\", 5: \"Random Forest Reg.\"}, \n",
    "                              axis = 1,\n",
    "                              inplace = True)\n",
    "\n",
    "# Setting the index to the X_test index\n",
    "# and sorting from lowest to highest\n",
    "\n",
    "org_model_predictions = org_model_predictions.set_index(X_test.index).sort_index(ascending = True)\n",
    "\n",
    "# Saving as a .csv file\n",
    "\n",
    "org_model_predictions.to_csv(\"../Data/original_model_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe out of the four sets of predictions\n",
    "# and the y_test true values for comparison\n",
    "\n",
    "eng_model_predictions = pd.DataFrame([np.array(y_test_eng), lr_2_preds, ridge_2_preds, \n",
    "                                     lasso_2_preds, enet_2_preds, rfr_2_preds])\n",
    "\n",
    "# Transposing the dataframe so it has five columns\n",
    "\n",
    "eng_model_predictions = eng_model_predictions.T\n",
    "\n",
    "# Renaming the columns for each model\n",
    "\n",
    "eng_model_predictions.rename({0: \"Actual\", 1: \"Linear Reg.\", 2: \"RidgeCV\",\n",
    "                              3: \"LassoCV\", 4: \"ElasticNetCV\", 5: \"Random Forest Reg.\"}, \n",
    "                              axis = 1,\n",
    "                              inplace = True)\n",
    "\n",
    "# Setting the index to the X_test index\n",
    "# and sorting from lowest to highest\n",
    "\n",
    "eng_model_predictions = eng_model_predictions.set_index(X_test.index).sort_index(ascending = True)\n",
    "\n",
    "# Saving as a .csv file\n",
    "\n",
    "eng_model_predictions.to_csv(\"../Data/engineered_model_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading In The Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features   = pd.read_csv(\"../Data/original_model_predictions.csv\", index_col = 0)\n",
    "engineered_features = pd.read_csv(\"../Data/engineered_model_predictions.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the df looks correct\n",
    "\n",
    "original_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the df look correct\n",
    "\n",
    "engineered_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply plots the predictions from all four models together with the true `y` values in one scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(df, true, title):\n",
    "    \n",
    "    # Setting the `facecolor` to white makes\n",
    "    # it easier to see on a different background.\n",
    "    plt.figure(figsize   = (18,6),\n",
    "               facecolor = \"white\")\n",
    "    \n",
    "    # Generating a scatter plot for each set of\n",
    "    # values, but they'll be on one chart.\n",
    "    plt.scatter(x = true, y = \"Actual\", data = df,\n",
    "                color = \"black\", label = \"Actual\")\n",
    "    plt.scatter(x = true, y = \"Linear Reg.\", data = df,\n",
    "                color = \"red\", label = \"Linear Reg.\")\n",
    "    plt.scatter(x = true, y = \"RidgeCV\", data = df,\n",
    "                color = \"blue\", label = \"RidgeCV\")\n",
    "    plt.scatter(x = true, y = \"LassoCV\", data = df,\n",
    "                color = \"green\", label = \"LassoCV\")\n",
    "    plt.scatter(x = true, y = \"ElasticNetCV\", data = df,\n",
    "                color = \"orange\", label = \"ElasticNetCV\")\n",
    "    \n",
    "    # Setting graph parameters\n",
    "    plt.title(f\"Predictions From The {title}\", size = 18)\n",
    "    plt.xlabel(\"Predicted\", size = 16)\n",
    "    plt.ylabel(\"Actual\", size = 16)\n",
    "    plt.xticks(size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.legend(bbox_to_anchor = (1.04,1), loc = \"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same as the plots in my first notebooks,\n",
    "# but graphing the residuals.\n",
    "\n",
    "def plot_residuals(df, columns, titles):\n",
    "    count = 0\n",
    "    fig   = plt.figure(figsize = (18,8))\n",
    "    for c, column in enumerate(columns):\n",
    "        count += 1\n",
    "        ax    = fig.add_subplot(2, 2, count)\n",
    "        plt.title(f\"{titles[c]}\", size = 18)\n",
    "        sns.residplot(x = \"Actual\",\n",
    "                      y = column,\n",
    "                      data = df)\n",
    "        plt.xlabel(\"Predicted\", size = 16)\n",
    "        plt.ylabel(\"Actual\", size = 16)\n",
    "        plt.xticks(size = 14)\n",
    "        plt.yticks(size = 14)\n",
    "    plt.tight_layout();\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we had a good idea of how the models were performing ased on the scores calculated by our metrics, we felt that visualizing the predictions of the model would help us have a better idea of where the models were strong and weak.  To do that, we simply plotting the four sets of predictions along with the true values in  a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(df    = original_features, \n",
    "                 true  = \"Actual\", \n",
    "                 title = \"Original Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four models performed the best on values <200 given that the points there are overlapping so much.  However for values >200, the predictions follow the actual values but they became much more disperse: the variance increased, but not an extreme amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(df    = engineered_features, \n",
    "                 true  = \"Actual\", \n",
    "                 title = \"Engineered Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models run on the engineered features were also very dense in the 100 to 200 range, but increasingly underperformed for values >300.  This is because the variance increases as the quantity of high values decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are simply the difference between the actual values and the predicted values.  Looking at the residuals allows us to detect if the independence of errors assumption is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(df      = original_features,\n",
    "               columns = [\"Linear Reg.\", \"RidgeCV\",\n",
    "                          \"LassoCV\", \"ElasticNetCV\"],\n",
    "               titles  = [\"Linear Regression\", \"RidgeCV\",\n",
    "                          \"LassoCV\", \"ElasticNetCV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residual plots are not at all surprising given the distribution of my predicted values: they are dense for low values and very disperse for high values.  That means that the data is heteroscedastic, which means that it is violating one of the assumptions of linear models: independence of errors; ideally the points would be scattered randomly along the horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(df      = engineered_features,\n",
    "               columns = [\"Linear Reg.\", \"RidgeCV\",\n",
    "                          \"LassoCV\", \"ElasticNetCV\"],\n",
    "               titles  = [\"Linear Regression\", \"RidgeCV\",\n",
    "                          \"LassoCV\", \"ElasticNetCV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residual plots are not at all surprising given the distribution of my predicted values: they are dense for low values and very disperse for high values.  It is easy to see where the models were underperforming for low valuess and high values. As a result, this data is also heterscedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance With The Four Metrics With The Original Features:\n",
    "    \n",
    "| Metric             | Linear Regression | RidgeCV | LassoCV | ElasticNetCV |\n",
    "|:-------------------|:-----------------:|:-------:|:-------:|:------------:|\n",
    "| RMSE               | 17.47             | 17.46   | 17.45   | 18.71        |\n",
    "| MAE                | 12.90             | 12.93   | 12.93   | 14.38        |\n",
    "| R<sup>2</sup>      | 0.9237            | 0.9238  | 0.9239  | 0.9125       |\n",
    "| Adj. r<sup>2</sup> | 0.9231            | -----   | -----   | -----        |\n",
    "\n",
    "\n",
    "Model Performance With The Four Metrics With The Engineered Features:\n",
    "\n",
    "| Metric             | Linear Regression | RidgeCV   | LassoCV | ElasticNetCV |\n",
    "|:-------------------|:-----------------:|:---------:|:-------:|:------------:|\n",
    "| RMSE               | 23.30             | 23.11     | 23.16   | 24.88        |\n",
    "| MAE                | 14.59             | 15.23     | 15.29   | 16.69        |\n",
    "| R<sup>2</sup>      | 0.8666            | 0.8666    | 0.8659  | 0.8454       |\n",
    "| Adj. R<sup>2</sup> | 0.8662            | -----     | -----   | -----        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
